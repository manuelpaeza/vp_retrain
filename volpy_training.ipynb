{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision Maskrcnn on Volpy Data\n",
    "\n",
    "Created by Changjia Cai, Erik Thompson, and Manuel Paez \n",
    "\n",
    "Date Created: August 28, 2024\n",
    "Date Updated: January 08, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional if you want qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "from skimage.draw import polygon2mask\n",
    "\n",
    "import torchvision\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "import colorsys\n",
    "import random\n",
    "import cv2\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import torchvision\n",
    "from torchvision.tv_tensors import Mask, BoundingBoxes\n",
    "# IMport the augmentation transforms using both apis (functional and transform)\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from utils import distance_masks, find_matches, nf_match_neurons_in_binary_masks, norm_nrg\n",
    "from utils import ScaleImage, vp_load_image, plot_volpy_segs, draw_bbox, bounding_box\n",
    "from utils import create_mask, box_area, normalize_image, random_colors, apply_mask\n",
    "from utils import apply_masks, draw_bbox, collate_fn, draw_bboxes, \n",
    "\n",
    "from model import get_model_instance_segmentation, thresholded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inspect VolPy training data (Optional)\n",
    "\n",
    "First, download the Volpy Training Data Files from here: \n",
    "\n",
    "Then, Set up directories as follows:\n",
    "\n",
    "    volpy_training_data/\n",
    "        images/\n",
    "            HPC.29.04.npz\n",
    "            ...\n",
    "        masks/\n",
    "            HPC.29.04_mask.npz\n",
    "            ...        \n",
    "            \n",
    "The following `data_dir` variable should be directed to `volpy_training_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users//data/volpy_training_data/' \n",
    "\n",
    "image_dir = data_dir + 'images/' \n",
    "image_path_list = list(sorted(os.listdir(os.path.join(data_dir, \"images\"))))\n",
    "mask_path_list = list(sorted(os.listdir(os.path.join(data_dir, \"masks\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some cursory inspection of one dataset. Note each image is three channels, but isn't RGB: channels one and two are mean, channel three is corr image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one image and masks\n",
    "dat_ind = 12\n",
    "data_path = image_path_list[dat_ind]  \n",
    "mask_path = mask_path_list[dat_ind] \n",
    "print(f'image name: {data_path}')\n",
    "print(f'mask name: {mask_path}')\n",
    "\n",
    "data_loaded = np.load(data_dir + 'images/' + data_path)\n",
    "masks_loaded = np.load(data_dir + 'masks/' + mask_path, allow_pickle=True)\n",
    "\n",
    "image = data_loaded['img']\n",
    "masks = masks_loaded['mask']\n",
    "image_gray = rgb2gray(image)\n",
    "num_masks = len(masks)\n",
    "print(f'image shape: {image.shape}, number of masks: {num_masks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.from_numpy(image).permute(2,0,1)\n",
    "image_tensor = tv_tensors.Image(image_tensor)\n",
    "max_v = 99\n",
    "outline_color = 'lime'\n",
    "outline_width = 0.75\n",
    "figsize = (4,8)\n",
    "\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=figsize, sharex=True, sharey=True)  # w/h\n",
    "\n",
    "# mean img\n",
    "ax1.imshow(image[:,:,1], cmap='gray', vmax=np.percentile(image[:,:,1], max_v))\n",
    "ax1.set_title('mean img')\n",
    "ax2.imshow(image[:,:,1], cmap='gray', vmax=np.percentile(image[:,:,1], max_v))\n",
    "for mask in masks:\n",
    "    ax2.plot(mask['all_points_x'], mask['all_points_y'], \n",
    "             color=outline_color, linewidth=outline_width)\n",
    "ax2.set_title('mean img masks')\n",
    "\n",
    "# corr img\n",
    "ax3.imshow(image[:,:,2], cmap='gray', vmax=np.percentile(image[:,:,2], max_v))\n",
    "ax3.set_title('corr img')\n",
    "ax4.imshow(image[:,:,2], cmap='gray', vmax=np.percentile(image[:,:,2], max_v))\n",
    "for mask in masks:\n",
    "    ax4.plot(mask['all_points_x'], mask['all_points_y'], \n",
    "             color=outline_color, linewidth=outline_width)\n",
    "ax4.set_title('corr img masks')\n",
    "plt.xlim([0, image.shape[1]-1])\n",
    "plt.ylim([0, image.shape[0]-1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Dataset\n",
    "Built on the `torch.utils.data.Dataset`: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "\n",
    "The  `__getitem__` method should return an `image` and a `target` dictionary delineating the different objects (box/mask) in the image:\n",
    "\n",
    "    image: torchvision.tv_tensors.Image of shape [3, H, W]: can be a pure tensor, or a PIL Image of size (H, W)\n",
    "    target: a dict containing the following keys\n",
    "        masks : torchvision uint8 binary masks for each object (N,H,W) (N masks)\n",
    "        boxes (bounding boxes)  (nx4)\n",
    "        labels (int) label for each bounding box (note 0 is background, so if you have no bg, start with 1)\n",
    "        image_id (int) unique image id\n",
    "        area (float) area of bounding box (I'm not sure this is technically required)\n",
    "        iscrowd (uint8) instances with `iscrowd=True` will be ignored during evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_scaler = ScaleImage()  \n",
    "\n",
    "class NeuralDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to ensure that they are aligned\n",
    "        self.image_filenames = list(sorted(os.listdir(os.path.join(self.root, \"images\"))))\n",
    "        self.mask_filenames = list(sorted(os.listdir(os.path.join(self.root, \"masks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = idx\n",
    "\n",
    "        # Image: (C x H x W)\n",
    "        image_path = os.path.join(self.root, \"images\", self.image_filenames[idx])\n",
    "        image = np.load(image_path)['img'] # mean/mean/corr channels  (h w c)\n",
    "        image = torch.from_numpy(image).permute(2,0,1) # convert to tensor and get into pytorch order C x H x W\n",
    "        image = image_scaler(image)   # scale so it is in 0,1 range\n",
    "        image = tv_tensors.Image(image)\n",
    "\n",
    "        # Masks: N x H x W mask array (N masks)\n",
    "        mask_path = os.path.join(self.root, \"masks\", self.mask_filenames[idx])\n",
    "        masks_loaded = np.load(mask_path, allow_pickle=True)\n",
    "        masks = masks_loaded['mask']\n",
    "        # first create boolean mask stack\n",
    "        all_masks = []\n",
    "        for mask_ind, mask_dict in enumerate(masks): # [mask_ind]\n",
    "            mask = create_mask(image[1].shape, mask_dict)\n",
    "            all_masks.append(mask)\n",
    "        all_masks = np.array(all_masks)\n",
    "        # then convert to binary uint8 tensor stack\n",
    "        all_masks = torch.from_numpy(all_masks.astype(np.uint8))\n",
    "                \n",
    "        boxes = masks_to_boxes(all_masks)\n",
    "        box_areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])  # tensor of areas\n",
    "\n",
    "        # there is only one class, so labels are all ones\n",
    "        num_objs = len(masks)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # let's just say nstances are not crowd: all instances will be used for evaluation\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap up everything into a dictionary describing target\n",
    "        target = {}\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"masks\"] = tv_tensors.Mask(all_masks)\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(image))\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = box_areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        # run augmentation, if transforms exist\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "            \n",
    "        return image, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "        \n",
    "    def print_image_filenames(self):\n",
    "        for image_filename in self.image_filenames:\n",
    "            print(image_filename)\n",
    "\n",
    "    def print_mask_filenames(self):\n",
    "        for mask_filename in self.mask_filenames:\n",
    "            print(mask_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volpy Training Details\n",
    "\n",
    "From the Volpy Paper:\n",
    "\n",
    "> During training, we randomly cropped the input image into *128x128 patches* and applied the following data augmentation techniques using the imgaug package:\n",
    "> \n",
    ">   `flip, rotation, multiply (adjust brightness), Gaussian noise, shear, scale and translation` \n",
    ">\n",
    "> Each mini-batch contained *six patches*. We trained on one GPU the head (the whole network except the ResNet) of the network for 20 epochs (2000 iterations) with learning rate 0.01 and then trained the head together with the last 28 layers of the ResNet for another 20 epochs with learning rate 0.001. \n",
    ">\n",
    "> We used stochastic gradient descent as our optimizer with a constant learning momentum 0.9. The weight decay was 0.0001. It is possible that correlations among RGB channels existing in the original COCO datasets are not present in our datasets, however retraining some of the ResNet layers is likely compensating for this potential issue. \n",
    ">\n",
    "> It roughly took 40 minutes to train 40 epochs on a GeForce RTX 2080 Ti GPU with 11 GB of RAM memory. During validation, images were padded with zeroes to make width and height multiples of 64 so that feature maps could be smoothly scaled for the Feature Pyramid Network. We only selected neurons with confidence level greater or equal to 0.7. \n",
    ">\n",
    "> The output components of the network were further filtered based on the number of pixels in each mask. For TEG datasets, masks containing less than 100 pixels were removed. For HPC datasets, masks containing pixels less than 400 were removed. For L1 datasets, there was no constraint on the number of pixels for each mask. VolPy segmentation performance is shown in Fig 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip, rotation, multiply (adjust brightness), Gaussian Noise, Shear, Scale and Translation\n",
    "def data_transform(train=False):\n",
    "    transform_pipeline = []\n",
    "    if train:\n",
    "        print(\"Generating training transform pipeline\")\n",
    "        transform_pipeline.append(T.ColorJitter(brightness=0.5,\n",
    "                                                contrast=0.5,\n",
    "                                                saturation=0.5,\n",
    "                                                hue=0.2))\n",
    "        transform_pipeline.append(T.GaussianBlur(kernel_size=(5,5), \n",
    "                                                         sigma=(0.001, 0.3)))  \n",
    "        transform_pipeline.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        transform_pipeline.append(T.RandomVerticalFlip(p=0.5))\n",
    "        # transform_pipeline.append(T.GaussianNoise())\n",
    "        transform_pipeline.append(T.RandomRotation(4, fill=0, expand=False))  \n",
    "        transform_pipeline.append(T.SanitizeBoundingBoxes(min_size=2))  \n",
    "        \n",
    "    # Convert to proper type and compose\n",
    "    transform_pipeline.append(T.ToDtype(torch.float32, scale=True))\n",
    "    transform_pipeline.append(T.ToPureTensor())\n",
    "    return T.Compose(transform_pipeline)\n",
    "\n",
    "print('transform updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Check: \n",
    "Set 'train=True.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_torch_workers = 0 # for windows set to 0, otherwise set to 4+ or whatever\n",
    "volpy_data = NeuralDataset(data_dir, data_transform(train=True))\n",
    "volpy_dataloader = torch.utils.data.DataLoader(volpy_data,\n",
    "                                               batch_size=6,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=num_torch_workers,\n",
    "                                               collate_fn=collate_fn)\n",
    "print(len(volpy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ind = 17\n",
    "orig_image, orig_image_path = vp_load_image(image_dir, image_path_list, session_ind) # orig data and masks\n",
    "dataset_image, dataset_target = volpy_data[session_ind] # data after transformation\n",
    "\n",
    "summary_mn = gray2rgb(normalize_image(dataset_image[1,:,:]))\n",
    "summary_corr = gray2rgb(normalize_image(dataset_image[2,:,:]))\n",
    "data_masks = dataset_target['masks']\n",
    "data_boxes = dataset_target['boxes']\n",
    "\n",
    "# visualize transformed data\n",
    "mask_color = (0, 1, 0)\n",
    "plot_box = True\n",
    "box_color = 'white'\n",
    "alpha = 0.001\n",
    "\n",
    "masked_mn_vp = apply_masks(summary_mn, data_masks, mask_color, alpha=alpha)\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(6, 12))\n",
    "ax1.imshow(orig_image[:,:,0], cmap='gray', vmax=np.percentile(orig_image[:,:,0], 99.5))\n",
    "ax1.set_title('mean orig')\n",
    "ax2.imshow(summary_mn[:, :, 0], cmap='gray',  vmax=np.percentile(summary_mn, 99.5))\n",
    "ax2.set_title('mean transformed')\n",
    "ax3.imshow(masked_mn_vp)\n",
    "ax2.set_title('mean transformed with masks')\n",
    "if plot_box:\n",
    "    ax3, rects = draw_bboxes(data_boxes, color=box_color, line_width=1, ax=ax3);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader Check\n",
    "\n",
    "Dataloader is an iterable. To create an iterator need to `iter(data_loader)`, then can run `next()` to pull the batch out of it that training is run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_targets = next(iter(volpy_dataloader))\n",
    "\n",
    "# standard way of extracting images and targets for a batch into a list\n",
    "batch_images = list(image for image in batch_images)\n",
    "batch_targets = [{k: v for k, v in t.items()} for t in batch_targets]\n",
    "\n",
    "# check one sample\n",
    "batch_ind = 0\n",
    "batch_item_target = batch_targets[batch_ind]\n",
    "batch_item_image = batch_images[batch_ind]\n",
    "batch_item_id = batch_item_target['image_id']\n",
    "\n",
    "# extract original image so we can compare\n",
    "image_orig, image_orig_path = vp_load_image(image_dir, image_path_list, batch_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(4,8))\n",
    "ax1.imshow(image_orig[:,:,0], cmap='gray')\n",
    "ax1.set_title('mean orig')\n",
    "ax2.imshow(batch_item_image[0,:,:], cmap='gray')\n",
    "ax2.set_title('mean dataloader')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for Training\n",
    "Create a model generator to be ready to be trained and evaluated on the custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "We will start from a pre-trained model trained on the coco dataset, and fine-tune the last layer.\n",
    "\n",
    "We have two classes (background and neuron). Split into train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "num_classes = 2\n",
    "num_test = 8\n",
    "\n",
    "# training and test datasets defined (this will determine which transforms are applied)\n",
    "dataset = NeuralDataset(data_dir, data_transform(train=True))\n",
    "dataset_val = NeuralDataset(data_dir, data_transform(train=False))\n",
    "\n",
    "# here we select two thirds of datasets for training, one third of datasets for testing\n",
    "# we make sure each type of datasets (L1, TEG, HPC) has at least one dataset for testing\n",
    "indices = [4, 5, 6, 7, 8, 9, 10, 11, 15, 16, 17, 18, 19, 20, 22, 23, 0, 1, 2, 3, 12, 13, 14, 21]\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-num_test])\n",
    "dataset_val = torch.utils.data.Subset(dataset_val, indices[-num_test:])\n",
    "\n",
    "print(f'training sets: {indices[:-num_test]}')\n",
    "print(f'test sets: {indices[-num_test:]}')\n",
    "\n",
    "num_torch_workers = 0  # this is necessary for windows. In other OS set to 4+\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=2,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_torch_workers,\n",
    "                                          collate_fn=collate_fn);\n",
    "data_loader_val = torch.utils.data.DataLoader(dataset_val,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=num_torch_workers,\n",
    "                                               collate_fn=collate_fn)\n",
    "\n",
    "print(f'number of batches per epoch for training {len(data_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model using our helper function and move to the correct device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(num_classes)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up optimizer/lr scheduler\n",
    "\n",
    "Note: If the learning rate is too high initially, you can get NaNs and error during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 0.005\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params,\n",
    "                            lr=max_lr, # 0.003\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=0.0001) #0.0005\n",
    "optimizer.param_groups[0][\"lr\"]  # to extract the learning rate\n",
    "\n",
    "lr_scheduler = CyclicLR(optimizer, \n",
    "                     base_lr = 0.000001,     # Initial learning rate which is the lower boundary in the cycle for each parameter group\n",
    "                     max_lr = max_lr,       # 0.003 Upper learning rate boundaries in the cycle for each parameter group\n",
    "                     step_size_up = 3,     # Number of training iterations in the increasing half of a cycle\n",
    "                     step_size_down = 7,\n",
    "                     mode = \"triangular2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = 'C:/Users/Eric/development/data/volpy_models/'\n",
    "try:\n",
    "    os.mkdir(model_save_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "num_epochs = 100\n",
    "print_freq = 1\n",
    "save_freq = 20\n",
    "num_train = len(dataset)\n",
    "num_val = len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "all_lrs = []\n",
    "\n",
    "print(f\"**TRAIN {num_epochs} epochs. PRINT every {print_freq} epoch(s). SAVE every {save_freq} epoch(s).**\")\n",
    "\n",
    "for epoch in range(num_epochs):      \n",
    "    train_epoch_loss = 0\n",
    "    val_epoch_loss = 0\n",
    "\n",
    "    ### TRAIN STEP\n",
    "    if np.mod(epoch+1, print_freq) == 0:\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "    #    print(\"\\tTraining step...\")\n",
    "        \n",
    "    model.train()\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum([loss for loss in loss_dict.values()])\n",
    "        train_epoch_loss += losses.cpu().detach().numpy()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    all_train_losses.append(train_epoch_loss)\n",
    "    all_lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Validation STEP\n",
    "    #if np.mod(epoch+1, print_freq) == 0:\n",
    "    #    print(\"\\tTest step...\")\n",
    "    with torch.no_grad():\n",
    "        for j, (images_val, targets_val) in enumerate(data_loader_val):\n",
    "            images_val = list(image.to(device) for image in images_val)\n",
    "            targets_val = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets_val]\n",
    "            loss_val = model(images_val, targets_val)\n",
    "            losses_val = sum([l for l in loss_val.values()])\n",
    "            val_epoch_loss += losses_val.cpu().detach().numpy()\n",
    "        all_val_losses.append(val_epoch_loss)\n",
    "        \n",
    "    if np.mod(epoch+1, print_freq) == 0:\n",
    "        print(\"\\ttrain/test net loss: \", train_epoch_loss, \" \", val_epoch_loss)\n",
    "\n",
    "    # Save model (if save)\n",
    "    if np.mod(epoch+1, save_freq) == 0:\n",
    "        print(f\"\\tSaving mrcnn epoch {epoch+1}\")\n",
    "        model_fname = 'mrcnn_' + str(epoch+1) + '.pt'\n",
    "        model_path = model_save_dir + model_fname\n",
    "        torch.save(model, model_path)\n",
    "\n",
    "    torch.cuda.empty_cache()  # may help with memory management\n",
    "        \n",
    "history = [all_train_losses, all_val_losses, all_lrs]\n",
    "torch.save(history, model_save_dir + 'volpy_train_history.pt')\n",
    "print(\"\\nDONE!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss function\n",
    "plt.plot(np.array(all_train_losses)/num_train, color='blue', marker='.', label='train')\n",
    "plt.plot(np.array(all_val_losses)/num_test, color='red', marker='.', label='validation')\n",
    "plt.legend();\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('net loss')\n",
    "plt.title('loss function across different epochs')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning rate vs epoch:\n",
    "plt.plot(all_lrs, marker='.')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('learning rate')\n",
    "plt.title('learning rate across different epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull original data \n",
    "idx = 4\n",
    "val_set = np.array([0, 1, 2, 3, 12, 13, 14, 21])\n",
    "volpy_inference_dataset = NeuralDataset(data_dir, data_transform(train=False))\n",
    "vp_im, vp_target = volpy_inference_dataset[val_set[idx]]\n",
    "summary_mn = gray2rgb(normalize_image(vp_im[1,:,:]))\n",
    "summary_corr = gray2rgb(normalize_image(vp_im[2,:,:]))\n",
    "data_masks = vp_target['masks']\n",
    "data_boxes = vp_target['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference on one dataset\n",
    "eval_transform = data_transform(train=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(vp_im)\n",
    "    x = x.to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = predictions[0]\n",
    "\n",
    "# prediction score range\n",
    "print(f'prediction scores: {pred[\"scores\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "predicted_masks, predicted_boxes = thresholded_predictions(pred, threshold=thresh) \n",
    "predicted_masks_np = (((0.5+predicted_masks).detach().cpu().numpy())).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get masked image of ground truth and predictions\n",
    "mask_alpha = 0.001\n",
    "mask_color = (0,1,0)\n",
    "masked_vp = apply_masks(summary_mn, data_masks, color=mask_color, alpha=mask_alpha) \n",
    "masked_pred = apply_masks(summary_mn, predicted_masks_np, color=mask_color, alpha=mask_alpha)\n",
    "masked_pred = masked_pred/masked_pred.max()\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(8,8), sharex=True, sharey=True)\n",
    "ax1.imshow(summary_mn)\n",
    "ax2.imshow(masked_vp)\n",
    "ax3.imshow(masked_pred)\n",
    "if plot_box:\n",
    "    ax2, rects = draw_bboxes(data_boxes, ax=ax2, line_width=1, color='white');\n",
    "    ax3, rect = draw_bboxes(predicted_boxes.detach().cpu().numpy(), ax=ax3, color='white', line_width=1);\n",
    "ax1.set_title('orig')\n",
    "ax2.set_title('ground Truth')\n",
    "ax3.set_title('Mask R-CNN')\n",
    "plt.suptitle('mean image overlaid with masks', fontsize=15)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score for this dataset\n",
    "_, _, _, _, performance = nf_match_neurons_in_binary_masks(data_masks.numpy().astype(np.float64), \n",
    "                                 predicted_masks_np.astype(np.float64), \n",
    "                                 plot_results=True, Cn=summary_mn, \n",
    "                                 labels=['GT', 'VolPy'], \n",
    "                                 colors=['red', 'yellow'])\n",
    "print(f\"F1 score: {performance['f1_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score on all test sets\n",
    "val_set = np.array([0, 1, 2, 3, 12, 13, 14, 21])\n",
    "eval_transform = data_transform(train=False)\n",
    "F1_all = []\n",
    "\n",
    "for idx in val_set:\n",
    "    vp_im, vp_target = volpy_inference_dataset[idx]\n",
    "    summary_mn = gray2rgb(normalize_image(vp_im[1,:,:]))\n",
    "    data_masks = vp_target['masks']\n",
    "    _, _, binarized_masks = mrcnn_inference(model, img=vp_im, thresh=0.5)    \n",
    "\n",
    "    try:\n",
    "        _, _, _, _, performance = nf_match_neurons_in_binary_masks(data_masks.numpy().astype(np.float64), \n",
    "                                         binarized_masks.astype(np.float64), \n",
    "                                         plot_results=True, Cn=summary_mn, \n",
    "                                         labels=['GT', 'VolPy'], \n",
    "                                         colors=['red', 'yellow'])\n",
    "        print(f\"F1 score: {performance['f1_score']}\")\n",
    "        F1_all.append(performance['f1_score'])\n",
    "        \n",
    "    except:\n",
    "        print(0)\n",
    "        F1_all.append(0)\n",
    "        \n",
    "\n",
    "F1_all = np.array(F1_all)    \n",
    "print(f'avg F1 score for HPC: {F1_all[:4].mean()}')\n",
    "print(f'avg F1 score for L1: {F1_all[4:7].mean()}')\n",
    "print(f'avg F1 score for TEG: {F1_all[7]}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
